// scripts/sync_exploitdb.js
import { createClient } from '@supabase/supabase-js';
import fetch from 'node-fetch';
import 'dotenv/config';

// --- Config ---
// [FINAL-FIX-URL-ExploitDB] Using the direct CSV download link from exploit-db.com
const EXPLOITDB_CSV_URL = 'https://www.exploit-db.com/exploits.csv'; // <--- آدرس جدید و صحیح
const SUPABASE_URL = process.env.SUPABASE_URL;
const SUPABASE_SERVICE_KEY = process.env.SUPABASE_SERVICE_KEY;

if (!SUPABASE_URL || !SUPABASE_SERVICE_KEY) {
  console.error('::ERROR:: SUPABASE_URL or SUPABASE_SERVICE_KEY is not defined.');
  process.exit(1);
}

const supabase = createClient(SUPABASE_URL, SUPABASE_SERVICE_KEY, {
  auth: {
      persistSession: false
  }
});

function parseCSVLine(line, headers) {
  // Improved regex to handle potentially tricky CSV fields
  const values = [];
  let currentVal = '';
  let inQuotes = false;
  for (let i = 0; i < line.length; i++) {
    const char = line[i];
    if (char === '"' && line[i+1] === '"') { // Handle escaped quotes ""
      currentVal += '"';
      i++; // Skip next quote
    } else if (char === '"') {
      inQuotes = !inQuotes;
    } else if (char === ',' && !inQuotes) {
      values.push(currentVal.trim());
      currentVal = '';
    } else {
      currentVal += char;
    }
  }
  values.push(currentVal.trim()); // Add the last value

  let obj = {};
  headers.forEach((header, index) => {
    // Clean header names (remove potential quotes)
    const cleanHeader = header.replace(/"/g, '');
    obj[cleanHeader] = values[index] || ''; // Use cleaned header
  });
  return obj;
}

function transformExploitData(raw) {
    // Adjust field names based on the actual headers from exploit-db.com's CSV
  const id = raw.id;
  const title = raw.description; // 'description' seems to be the title field
  const datePublished = raw.date_published;
  const author = raw.author;
  const type = raw.type;
  const platform = raw.platform;

  if (!id || !title || isNaN(parseInt(id, 10))) {
    console.warn(`::WARN:: Skipping invalid row: ${JSON.stringify(raw)}`);
    return null;
  }
  
  let isoDate;
  try {
      // Handle potential date format variations if necessary
      isoDate = new Date(datePublished).toISOString();
  } catch (e) {
      console.warn(`::WARN:: Could not parse date "${datePublished}" for exploit ${id}. Using current date.`);
      isoDate = new Date().toISOString();
  }

  return {
    id: parseInt(id, 10),
    title: title,
    date: isoDate,
    author: author,
    type: type,
    platform: platform,
    url: `https://www.exploit-db.com/exploits/${id}`
  };
}


async function main() {
  console.log('::JOB_START:: Starting Exploit DB sync process...');
  console.log(`::FETCH:: Attempting to fetch: ${EXPLOITDB_CSV_URL}`);
  let csvText;
  try {
    const response = await fetch(EXPLOITDB_CSV_URL);
    if (!response.ok) {
      throw new Error(`Failed to fetch CSV: ${response.statusText} (URL: ${EXPLOITDB_CSV_URL})`);
    }
    csvText = await response.text();
    console.log('::SUCCESS:: Fetched Exploit DB CSV.');
  } catch (error) {
    console.error('::FETCH_ERROR::', error.message);
    process.exit(1);
  }

  const lines = csvText.split('\n').filter(Boolean);
  if (lines.length < 2) {
      console.error('::ERROR:: CSV file seems empty or invalid.');
      process.exit(1);
  }
  
  // Dynamically get headers from the first line
  const headers = lines[0].split(',').map(h => h.replace(/"/g, '').trim());
  lines.shift(); // Remove header line
  
  console.log(`::PROCESS:: Found ${lines.length} exploits to process...`);
  console.log(`::INFO:: CSV Headers: ${headers.join(', ')}`); // Log headers for debugging

  const dataToUpsert = lines
    .map(line => parseCSVLine(line, headers))
    .map(transformExploitData) // Use the correct function name
    .filter(Boolean); // Filter out null results

  if (dataToUpsert.length === 0) {
    console.log('::INFO:: No valid exploit data found to upsert.');
    return;
  }

  console.log(`::DB_SYNC:: Upserting ${dataToUpsert.length} valid records in chunks...`);
  const CHUNK_SIZE = 500;
  for (let i = 0; i < dataToUpsert.length; i += CHUNK_SIZE) {
    const chunk = dataToUpsert.slice(i, i + CHUNK_SIZE);
    const { error } = await supabase
      .from('exploits')
      .upsert(chunk, {
        onConflict: 'id',
        ignoreDuplicates: false,
      });
    if (error) {
      console.error(`::DB_ERROR:: Failed to upsert chunk ${Math.floor(i / CHUNK_SIZE) + 1}:`, error.message);
      // Optional: Log the chunk data that failed
      // console.error("Failed chunk data:", JSON.stringify(chunk.slice(0, 5), null, 2)); // Log first 5 items
    } else {
      console.log(`::DB_SUCCESS:: Upserted chunk ${Math.floor(i / CHUNK_SIZE) + 1} (${chunk.length} records).`);
    }
  }
  console.log('::JOB_END:: Exploit DB sync process finished.');
}

main().catch(err => {
  console.error('::FATAL_ERROR::', err);
  process.exit(1);
});

